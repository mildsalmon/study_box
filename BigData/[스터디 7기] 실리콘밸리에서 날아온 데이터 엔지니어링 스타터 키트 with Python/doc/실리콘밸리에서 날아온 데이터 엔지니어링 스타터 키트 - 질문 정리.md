# Table of Contents

- [1. 1주차](#1-1주차)
- [2. 2주차](#2-2주차)

---

# 1. 1주차

1. 아까 온프레미스 보다는 클라우드 쪽이 더 우선으로 얘기해주셨는데,  지금까지 경험한 취준 경험들이 전부 클러스터가 직접 구축되고 하둡이나 스파크를 쓰는 온프레미스 환경이라서 이제는 이 부분들이 취준관점에 있어 경쟁력에 뒤지게 되어서 클라우드 위주로 다시 공부해야하는게 맞을지 궁금합니다.
	- 하둡이나 스파크 지식은 도움이 된다.
	- 클라우드를 새로 배운다고 생각하지 말고, 연장선상이라 생각해라.
2. 본 수업에서 Spark / Hadoop도 다루게 되나요?
	- 아니요.
3. 그럼 데브옵스 직군은 구축보다 유지보수 업무에 조금 더 집중되어 있나요?
	- 포커스가 개발보다는 개발자들이 만든 코드를 안정적으로 production에 배포, 서비스가 안정적으로 돌고 있는지 모니터링, 문제가 발견됬을 경우 어떻게 대응할 것인지 등의 유지보수에 포커스가 맞춰져 있다.
4. 강사님 질문이 있습니다. DW의 경우에는 대부분 RDB를 사용한다고 보면 될까요. 데이터엔지니어 직군의 경우 JD에 NoSQL 스킬을 필요로하는 경우도 많이 보아서, 이런 경우 NoSQL은 특정 서비스에 한정되어 개별적으로 관리되는 데이터베이스로 보아도 무방할까요?
	- DW 자체는 RDB이다.
		- BigQuery, Snowflake, Redshift 모두 RDB
	- RDB이지만, 큰 데이터를 처리할 수 있는 RDB
	- NoSQL이 JD에 있는 경우는 머신러닝 엔지니어나 데이터 사이언티스트와 일을 많이 하는 경우임.
		- 모델을 deploy할 때, 모델에서 필요한 feature들을 저장하기 위해 NoSQL 사용
5. 강의에서 말씀해주신 대로, 현업에서는 사람이 튜닝해야하는 포인트를 줄이고 다른 업무에 집중하기 위해서 많은 비용을 감수하고 redshift나 snowflake 같은 매니지드 서비스를 많이 사용합니다. 하지만 반대로 오픈 소스를 사용했을때에 비해서 직접 해볼수 있는 것이 적다는 단점도 있을 것 같습니다. 또한, 매니지드 서비스의 경우 자세한 로직들이나 코드가 공개되어있지 않아서 공부할 포인트가 오픈소스에 비해서 적다는 면도 있어서 이게 주니어에게 마냥 좋지는 않을수도 있다고 생각하는데, snowflake 같은 매니지드 서비스를 사용하고 있는 팀에서 일하고 있다면, 커리어를 위해서 어떤 부분에 집중해서 공부를 하는게 좋을까요?
	- 내부 동작 방식을 이해하고 기술적인 부분을 더 아는 것도 중요하다.
		- 다만, tensorflow가 나오고 머신러닝이 대중적으로 사용된 것처럼 내부 동작을 몰라도 상관없다.
		- 세상이 그렇게 바껴가는 것이라 생각하면 좋다.
	- DB를 사용하는 관점에서는 기본기가 중요하다.
		- **SQL**을 얼마나 효율적으로 작성할 수 있는가?
	- 데이터 엔지니어 관점에서는?
		- **python**으로 airflow를 얼마나 잘 작성할 수 있는가?
	- 커리어를 길게 보면, 기술적인 것에 집중하지 말고, 내가 어떻게 하면 좋은 결과를 낼 수 있는가에 포커스를 둬라.
		- 솔루션보다 결과나 문제에 포커스
6. 혹시 redshift에서 google sheet로 보내실 때 어떤 툴로 파이프라인 구축하셨는지 여쭤봐도 될까요..?  Google drive api가 조금 불안정한 것 같은데 작업하실때 문제 없으셨는지 궁금합니다.
	- 문제 없었음.
7. 시스템에 의해서 수집되는 형태가 아닌 사람에 의해 입력되어야 하는 정보는 어떤식으로 수집하는것이 좋을까요?
	- 사람이 입력하는 데이터는 google spreadsheet를 사용했다.
	- 극히 일부의 사람만 Edit할 수 있는 권한을 주고 그 spreadsheet로부터 정해진 시간(하루에 몇 번씩) 읽어서 Redshift의 table에 적재된다.
	- Redshift에 있는 table을 google spreadsheet로 복사하기도 한다.
8. 현 회사에서는 분석용도로 관계형 데이터베이스를 사용하지 않고, Presto로 Hive의 테이블들을 직접 조회하고 있습니다. (그래서 DW라는 용어가 아닌 Data Lake라는 용어를 사용하고 있고요.) 강의에서 관계형 데이터베이스를 구축하지 않으면 손이 더 많이 가서 더 비용이 많이 발생한다고 하셨는데, 어떤 점 때문에 그런지 좀더 자세히 설명해 주실 수 있을까요? 전 회사에서는 분석용도로 Redshift를 사용했었는데, Hive의 테이블들을 직접 조회하는 Presto로 이관할 예정이니 대비하라고 했었어서, Hive + Presto 조합이 최신 트렌드인가 하고 막연하게 생각했었거든요. ETL에서, Production DB/3rd Party 데이터를 중앙 데이터인프라에 Copy해오는 ETL 과정은 코딩 + Airflow로 진행한다고 말씀 주셨는데요, 이 프로세스를 도와주는 외부 툴을 활용하기도 하나요?
	- Hive가 도는 hadoop cluster가 on-premise에 있기 때문에 Hive cluster를 관리하는 비용이 적지 않게 들어간다.
		- 그래서 Hive와 다른 DW의 이슈가 아니라, 지금 Hive가 돌아가는 환경이 cloud인지 아닌지가 더 중요하다.
	- coding + airflow는 summary table을 만들게 되는데, DBT를 사용하기도 한다.

# 2. 2주차

1. s3에서 redshift로 copy할 때 스키마는 어떻게 맞추나요?
	- 보통 copy하는 것이 csv, json 등 schema가 들어있는 binary 파일이다. 그래서 해당하는 table이 이미 만들어져 있어야 한다.
	- copy하는 명령이 table을 알아서 만들지는 않음.
	- json이나 binary 파일 등 schema가 있는 파일은 필드로 알아서 매칭이 된다.
	- csv 파일은 csv 파일의 필드 순서와 테이블의 필드 순서가 동일해야 한다.
2. Redshift는 대용량적제 및 처리가 가능한 DB라고 봐도 무방할까요?
	- ok
	- Redshift를 사용하다가 Snowflake나 BigQuery로 넘어가는 것이 그렇게 어렵지 않다.
3. OLTP의 느낌을 서비스 사용자들의 행동이나 기록을 저장해 놓기때문에, 속도와 같은 성능에 더욱 포커스가 간다고 하셨는데 사실 직접 와닿지가 않습니다. 혹시, 이해하기 쉬운 예시가 있을지 궁금합니다.
	- OLTP
		- 카톡으로 메시지 보내면, 메시지 보낸 정보, 시간 등을 저장해야한다.
		- 이런 저장을 하는데 사용하는 DB가 OLTP
		- 속도가 중요
		- 복잡한 것을 기록하는 것이 아님
		- 웹 프로그래밍을 할 때 사용되는 DB
		- 사용자와 직접 facing하는 서비스 개발에 사용되는 DB
	- OLAP
		- 굉장히 큰 데이터를 가지고 JOIN을 해서 분석을 위한 것
		- 데이터 분석을 위한 DB